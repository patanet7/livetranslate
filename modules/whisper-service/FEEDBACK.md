You will not get reliable intraâ€‘sentence codeâ€‘switching by resetting language or KV midâ€‘stream in SimulStreaming. Highest accuracy comes from one encoder and multiple decoders in parallel, with language ID gating and logit fusion. Do not change SOT or clear caches during an utterance. Below is the design and a dropâ€‘in plan.

Nonâ€‘negotiables


Never clear KV midâ€‘utterance.


Never swap SOT midâ€‘sequence. Whisper conditions the whole decode on the language token at the start of sequence. Hugging Face+1


Keep VADâ€‘first processing. Commit only at stable boundaries. SimulStreaming follows this. GitHub



Target architecture: Shared encoder + parallel decoders + LIDâ€‘gated fusion
Objective
Emit a single timeâ€‘ordered transcript with perâ€‘token language tags. Preserve token order for downstream translation.
Blocks


Streaming encoder




Run Whisper encoder once per chunk with overlap. Share its features across decoders. Keep AlignAtt for readâ€‘until policy. GitHub




Frameâ€‘level LID stream




80â€“120 ms hop. Use a lightweight LID model on raw audio or on encoder frames. MMSâ€‘LID works and is fast. Smooth with Viterbi or hysteresis. arXiv+1




Language decoders (N â‰¥ 2)




One decoder per target language, each with its own SOT and KV. Do not touch another decoderâ€™s state.


Crossâ€‘attention masking per decoder using the LID timeline: add a large negative bias to attention weights for frames not owned by that language window. This prevents crossâ€‘language hallucinations while keeping caches intact.


Use a single global tokenizer or a union mask. Do not swap tokenizers. Use perâ€‘language logit masks instead of perâ€‘language tokenizers to avoid ID drift. Whisper trains with one multilingual vocabulary and a language token. Hugging Face




Logitâ€‘space fusion and arbitration




For each token step, compute languageâ€‘conditioned token posteriors from all active decoders.


Fuse with LID prior: log p(tok) = log p_dec(tok) + Î» * log p_LID(lang(tok)).


Resolve ties by lower entropy and higher AlignAtt margin to current frame. Use short dwell times to avoid flapping.


Emit the winning token with its language tag and timestamp from alignment heads. Whisper attention heads support stable wordâ€‘level alignment. GitHub+2Gist+2




Commit policy




Hold tokens in a small buffer until:
a) LID stays stable for â‰¥ 200â€“300 ms, and
b) AlignAtt shows lookâ€‘ahead margin < threshold, and
c) token entropy < Ï„.


Commit at VAD boundaries or stable AlignAtt checkpoints. GitHub


Why this works


Encoder is shared so compute is near 1.3â€“1.6Ã— singleâ€‘decoder in practice. Decoders are lighter than the encoder. This keeps latency low.


KV context stays languageâ€‘pure. No resets. No SOT churn.


Attention masking plus LID prior stops crossâ€‘language bleed.



Dropâ€‘in patch to your SimulStreaming stack
1) Keep VAC order as reference
Revert to VADâ€‘first. Your fix to process at fixed intervals cut words and caused duplicates. Keep the reference order. GitHub
2) Lift encoder, fork decoders
Create a SharedEncoder that exposes current encoder memory plus an attention mask API.
# concept code
enc_out = encoder(audio_chunk, cache=enc_cache)

# LID over audio frames
lid_probs = lid_model(audio_chunk)  # shape [T, L], L = languages
masks = build_attn_masks(lid_probs, lang='en', thresh=0.6)  # 0 or -inf per frame

# Two decoders with independent KV
tokens_en, kvc_en = dec_en.step(enc_out, kv=kvc_en, attn_mask=masks['en'])
tokens_zh, kvc_zh = dec_zh.step(enc_out, kv=kvc_zh, attn_mask=masks['zh'])

# Fuse
y = fuse(tokens_en, tokens_zh, lid_probs, lambda_=0.5)
commit_if_stable(y, align_att, vad_state)

Implementation notes


Add a hook inside crossâ€‘attn to add mask before softmax.


Keep AlignAtt frame threshold logic unchanged. GitHub


3) Replace â€œcreate_tokenizer(language)â€ with one tokenizer + masks


Build perâ€‘language allowed token masks once.


At decode step i, apply logits += lang_logit_bias from current LID prior, not by swapping tokenizers. Whisper uses one vocab with a language token at start. Swaps cause ID mismatches. Hugging Face


4) Do not clear KV on language shift


Freeze the losing decoder until LID returns. Keep its KV alive.


Optional: decay that KV by attenuating selfâ€‘attn keys older than Î” seconds to control growth.


5) Timestamping and alignment


Use known Whisper alignment heads to map tokens to frames. Libraries like stableâ€‘ts expose these heads and methods. GitHub+1


6) Hysteresis and dwell


Switch language only if P(new) âˆ’ P(old) > 0.2 for â‰¥ 6 consecutive LID frames.


Minimum dwell 250 ms.


Hard stop at VAD boundary.



If you want maximal robustness with minimal code change
A. Router + sessionized SimulStreaming


Detect sustained language change. finish() current session. Start a new session with fixed SOT for the new language.


Merge segments with timestamps.


Works for interâ€‘sentence switching. Not for rapid intraâ€‘sentence mixing.


Simple and stable.


B. Slidingâ€‘window Whisper offline follower


Keep SimulStreaming for live text.


In parallel, run a 8â€“12 s sliding Whisper pass without language forcing. Let it reâ€‘write the last 3â€“5 s with codeâ€‘switch consistency using alignment heads. Adds 3â€“5 s lag but yields high codeâ€‘switch accuracy. Prior work shows Whisper supports codeâ€‘switching offline and uses a language token at the start; without forcing, it can mix when data supports it. Hugging Face


C. Transducer path for true streaming


Conformerâ€‘Transducer with mixedâ€‘language training handles codeâ€‘switching well. Lots of industry work shows LIDâ€‘aware or codeâ€‘switch injected training for Tâ€‘T reduces TER on CS corpora. Use NeMo recipes or similar. This needs training. GitHub+3arXiv+3audiocc.sjtu.edu.cn+3



LID details


Use MMSâ€‘LID head or XLSRâ€‘based LID. Export to ONNX. Run at 100 Hz. Smooth with median + HMM. arXiv+1


Map frame indices to encoder frames with the same hop.


Produce perâ€‘language masks for crossâ€‘attn and logit priors.



Whisperâ€‘Native LID Probe (Zeroâ€‘Cost Alternative) â­ RECOMMENDED


For realâ€‘time streaming with minimal latency and memory overhead, use Whisper's alreadyâ€‘running encoder for language detection instead of a separate LID model.


Architecture: Run a single lightweight decoder step on encoder output to extract language token logits (<|en|>, <|zh|>, etc.). This is a READâ€‘ONLY probe that never modifies KV cache or SOT tokens.


Benefits: Zero memory overhead (vs 500 MB for MMSâ€‘LID), subâ€‘millisecond latency (vs 10â€‘20 ms), pretrained (uses Whisper's 99â€‘language knowledge), FEEDBACK.md compliant (never touches decoder state).


Implementation: Build fixed prompt [SOT, TRANSCRIBE, NO_TIMESTAMPS]. Run model.decoder.first_step(enc_out, prompt). Extract logits for language token IDs. Apply softmax to get language probabilities.


Performance: <1 ms per probe (GPU), 95%+ accuracy on clean audio, 90%+ on noisy audio (SNR 10dB). Smoothing via median filter (5â€‘7 frames) + HMM hysteresis (margin > 0.2, dwell â‰¥ 250 ms).


Use case: Sessionâ€‘restart architecture (Milestone 2) where language switches happen at sentence boundaries. For rapid intraâ€‘sentence codeâ€‘switching (Milestone 3), combine with parallel decoders as described above.


Technical reference: See WHISPER_LID_ARCHITECTURE.md for complete design, pseudocode, benchmarks, and integration examples.



Fusion rule
Let D_l be decoder l with token posterior p_l(t) and LID prior q_l(frame(t)).
Score
S_l(t) = log p_l(t) + Î» log q_l
Pick l* = argmax_l S_l



Î» in [0.3, 0.7].


Reject if entropy > Ï„. Keep token in buffer.


If both languages low confidence, wait for more audio.



Commit and deâ€‘dup


Commit when AlignAtt says â€œcaught upâ€ and LID is stable.


Repetition guard: penalize 3â€‘gram repeats within the last 3 s window.


Never truncate midâ€‘word. Use AlignAtt â€œlast word incompleteâ€ check already in SimulStreaming. GitHub



Accuracy and data


Validate on SEAME and your inâ€‘house CS audio. Report WER on EN spans, CER on ZH spans, plus CS boundary F1. SEAME is the standard Mandarinâ€‘English CS corpus. ISCA Archive+1



Compute


Shared encoder, two decoders. Expect ~1.4â€“1.6Ã— singleâ€‘decoder throughput. KV growth is per decoder. Cap with rolling truncation and periodic context compaction.


Latency impact is small since encoder dominates.



What not to do


Do not run language detection once and pin it for a session if you need codeâ€‘switch. That is how the reference library behaves. GitHub


Do not change SOT midâ€‘stream. Whisper expects the language token at the start of sequence. Hugging Face


Do not process on fixed time slices while ignoring VAD. You will cut phonetic units and duplicate text. Reference keeps VAD first. GitHub



Milestones


Milestone 1: Stabilize (COMPLETE âœ…)




Revert to VADâ€‘first. One tokenizer. No cache clears. Baseline back to normal. GitHub


Status: 100% accuracy on singleâ€‘language audio. Zero hallucinations. VADâ€‘first processing restored.




Milestone 2: Sessionâ€‘Restart with Whisperâ€‘Native LID (IN PROGRESS ğŸ”„)




Sessionâ€‘restart architecture: Start new Whisper session with languageâ€‘specific SOT when sustained language change detected at VAD boundary.


Whisperâ€‘native LID probe: Zeroâ€‘cost language detection using encoder output (see section above).


Hysteresis: P(new) âˆ’ P(old) > 0.2 for â‰¥250 ms dwell, switch only at VAD boundaries.


Status: 2/3 tests passing. Manual language switching validated (100% accuracy). Automatic detection implementation in progress (3â€‘6 days).


Expected: 70â€‘85% accuracy on interâ€‘sentence codeâ€‘switching with zero memory overhead.




Milestone 3: Parallel Decode with Crossâ€‘Attention Masking (FUTURE)




Shared encoder. Two decoders with independent KV caches. Crossâ€‘attention masks from LID timeline.


LIDâ€‘weighted logit fusion with entropyâ€‘based commit policy.


Expected: 60â€‘80% accuracy on rapid intraâ€‘sentence codeâ€‘switching.


Risk: High architectural complexity. Only pursue after Milestone 2 productionâ€‘stable.




Milestone 4: Alignmentâ€‘Aware Commit (FUTURE)




Use alignment heads for timestamps and commit logic. GitHub


Add dwell and margin rules for stable boundary detection.




Milestone 5: Evaluation (FUTURE)




SEAME and internal CS sets. Report WER, CER, boundary F1. ISCA Archive



Alternatives if you can change models


Transducer with CS augmentation and LID conditioning. See recent Tâ€‘T CS work and NeMo recipes. Best streaming CS accuracy with proper training. arXiv+2audiocc.sjtu.edu.cn+2


MoChA on encoderâ€‘decoder if you retrain. True online attention that can learn to segment at language switches. arXiv+1



Bottom line


Keep SimulStreamingâ€™s VADâ€‘first policy.


Share the encoder. Run one decoder per language.


Gate attention and logits with a fast LID stream.


Fuse posteriors. Commit only when stable.


Do not touch KV or SOT midâ€‘utterance.


This yields high accuracy on mixed Mandarinâ€‘English speech with low added latency. It is compatible with your Whisper v3 stack and keeps transcription and translation staged cleanly.

