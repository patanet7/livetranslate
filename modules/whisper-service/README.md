# LiveTranslate Whisper Service

**üé§ NPU-Optimized Speech-to-Text Processing Service**

Production-ready real-time speech-to-text transcription service with Intel NPU acceleration, advanced speaker diarization, and enterprise-grade WebSocket infrastructure.

## üöÄ Quick Start (Standalone)

Get the Whisper service running independently in under 5 minutes:

```bash
# Clone and navigate to whisper service
cd modules/whisper-service

# Option 1: Docker (Recommended)
docker-compose up --build -d

# Option 2: Local development
pip install -r requirements.txt
python src/main.py

# Option 3: NPU acceleration (Intel systems)
docker-compose -f docker-compose.npu.yml up --build -d

# Test the service
curl http://localhost:5001/health
curl -X POST -F "audio=@test_audio.wav" http://localhost:5001/transcribe/whisper-base
```

**Service will be available at:**
- **REST API**: http://localhost:5001
- **Health Check**: http://localhost:5001/health
- **WebSocket**: ws://localhost:5001 (for real-time streaming)
- **NPU Status**: http://localhost:5001/device-info

## üìã Prerequisites

### System Requirements
- **CPU**: Modern x86_64 with AVX support
- **RAM**: 2-8GB depending on model size
- **Storage**: 5GB for models and cache
- **Optional**: Intel NPU (Core Ultra) or NVIDIA GPU for acceleration

### Software Dependencies
```bash
# Python 3.9+
python --version

# Docker Desktop (for containerized deployment)
docker --version
docker-compose --version

# For NPU acceleration (Intel systems)
# OpenVINO Runtime 2024.x
# NPU drivers
```

## üõ†Ô∏è Installation & Setup

### Method 1: Docker Deployment (Recommended)

```bash
# 1. Navigate to whisper service directory
cd modules/whisper-service

# 2. Copy environment template
cp .env.example .env

# 3. Edit configuration (optional)
# Edit .env file to customize models, ports, etc.

# 4. Start the service
docker-compose up --build -d

# 5. Verify it's running
curl http://localhost:5001/api/health
```

### Method 2: Local Development Setup

```bash
# 1. Create Python virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download models (optional - auto-downloaded on first use)
python scripts/download_models.py --model whisper-small.en

# 4. Set environment variables
export WHISPER_MODEL=whisper-small.en
export WHISPER_DEVICE=cpu  # or 'npu' or 'gpu'
export PORT=5001

# 5. Start the service
python src/main.py
```

### Method 3: NPU Acceleration Setup

```bash
# For Intel NPU systems
cd modules/whisper-service

# Build NPU-optimized container
docker-compose -f docker-compose.npu.yml up --build -d

# Verify NPU is detected
curl http://localhost:5001/api/devices
```

## ‚öôÔ∏è Configuration

### Environment Variables

Create a `.env` file in the whisper-service directory:

```bash
# Service Configuration
PORT=5001
HOST=0.0.0.0
WORKERS=1

# Model Configuration
WHISPER_MODEL=whisper-small.en
WHISPER_DEVICE=auto  # auto, cpu, gpu, npu
MODEL_CACHE_DIR=./models

# Performance Settings
MAX_CONCURRENT_REQUESTS=10
ENABLE_GPU=true
ENABLE_NPU=true

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Integration (optional)
REDIS_URL=redis://localhost:6379
POSTGRES_URL=postgresql://user:pass@localhost:5432/whisper
```

### Model Selection

Available models (size vs. accuracy trade-off):
```bash
# Fastest, least accurate
WHISPER_MODEL=whisper-tiny.en      # ~40MB, ~3x real-time

# Balanced performance
WHISPER_MODEL=whisper-small.en     # ~250MB, ~2x real-time  

# Best accuracy
WHISPER_MODEL=whisper-medium.en    # ~750MB, ~1.5x real-time
WHISPER_MODEL=whisper-large-v3     # ~1.5GB, ~1x real-time
```

## üß™ Testing & Usage

### Health Check
```bash
curl http://localhost:5001/api/health
```

Expected response:
```json
{
  "status": "healthy",
  "service": "whisper-service",
  "version": "1.0.0",
  "device": "npu",
  "model": "whisper-small.en",
  "uptime": 123.45
}
```

### Single File Transcription
```bash
# Upload audio file for transcription
curl -X POST \
  -F "file=@your_audio.wav" \
  http://localhost:5001/transcribe/whisper-small.en

# With additional parameters
curl -X POST \
  -F "file=@your_audio.wav" \
  -F "language=en" \
  -F "task=transcribe" \
  http://localhost:5001/transcribe/enhanced/whisper-small.en
```

### Real-time Streaming
```python
import asyncio
import websockets
import json
import wave

async def stream_audio():
    uri = "ws://localhost:5001/ws"  # Updated WebSocket endpoint
    async with websockets.connect(uri) as websocket:
        # Authenticate (optional for guest access)
        await websocket.send(json.dumps({
            "type": "authenticate",
            "token": "guest"  # or valid JWT token
        }))
        
        # Join a session
        await websocket.send(json.dumps({
            "type": "join_session",
            "session_id": "my-session-123"
        }))
        
        # Configure streaming
        await websocket.send(json.dumps({
            "type": "transcribe_stream",
            "model": "whisper-small.en",
            "sample_rate": 16000,
            "chunk_size": 1024,
            "language": "auto"
        }))
        
        # Stream audio chunks
        with wave.open("audio.wav", "rb") as wav:
            while True:
                chunk = wav.readframes(1024)
                if not chunk:
                    break
                    
                # Send audio data
                await websocket.send(json.dumps({
                    "type": "transcribe_stream",
                    "audio_data": chunk.hex(),  # Convert to hex string
                    "session_id": "my-session-123"
                }))
                
                # Receive transcription
                response = await websocket.recv()
                result = json.loads(response)
                if result.get("type") == "transcription_result":
                    print(f"Transcription: {result['text']}")

asyncio.run(stream_audio())
```

### Enterprise WebSocket API (ws://localhost:5001/ws)

The service includes enterprise-grade WebSocket infrastructure with advanced features:

#### Connection Management Events
```javascript
// Establish connection
socket = new WebSocket('ws://localhost:5001/ws');

// Connection events
socket.onopen = function() {
    console.log('Connected to Whisper service');
};

socket.onclose = function() {
    console.log('Disconnected from Whisper service');
};
```

#### Authentication & Sessions
```javascript
// Guest authentication
socket.send(JSON.stringify({
    type: 'authenticate',
    token: 'guest'
}));

// User authentication with JWT
socket.send(JSON.stringify({
    type: 'authenticate',
    token: 'your-jwt-token-here'
}));

// Join transcription session
socket.send(JSON.stringify({
    type: 'join_session',
    session_id: 'session-123'
}));

// Leave session
socket.send(JSON.stringify({
    type: 'leave_session',
    session_id: 'session-123'
}));
```

#### Heartbeat & Connection Health
```javascript
// Heartbeat monitoring
socket.send(JSON.stringify({
    type: 'heartbeat',
    timestamp: Date.now()
}));

// Ping/Pong for connection testing
socket.send(JSON.stringify({
    type: 'ping',
    data: 'test-data'
}));

// Handle pong response
socket.onmessage = function(event) {
    const message = JSON.parse(event.data);
    if (message.type === 'pong') {
        const rtt = Date.now() - message.original_timestamp;
        console.log(`Round-trip time: ${rtt}ms`);
    }
};
```

#### Session Persistence & Reconnection
```javascript
// Reconnect to existing session
socket.send(JSON.stringify({
    type: 'reconnect_session',
    session_id: 'previous-session-id',
    last_message_id: 'msg-123'  // Optional: resume from specific point
}));

// Get session information
socket.send(JSON.stringify({
    type: 'get_session_info',
    session_id: 'session-123'
}));

// Buffer message for session persistence
socket.send(JSON.stringify({
    type: 'buffer_message',
    session_id: 'session-123',
    message: {
        type: 'transcription_result',
        text: 'Hello world',
        confidence: 0.95
    },
    priority: 1  // 1=high, 5=normal, 10=low
}));
```

#### Advanced Features
```javascript
// Subscribe to events
socket.send(JSON.stringify({
    type: 'subscribe_events',
    events: ['transcription_complete', 'error', 'session_update']
}));

// Message routing
socket.send(JSON.stringify({
    type: 'route_message',
    target: 'translation-service',
    message: {
        text: 'Hello world',
        target_language: 'Spanish'
    }
}));

// Unsubscribe from events
socket.send(JSON.stringify({
    type: 'unsubscribe_events',
    events: ['error']
}));
```

### Complete REST API Reference

#### Health & Status
```bash
# Basic health check
GET /health

# CORS test endpoint
GET|POST|OPTIONS /cors-test

# Service status with metrics
GET /status

# Connection statistics
GET /connections

# Error statistics
GET /errors

# Heartbeat monitoring stats
GET /heartbeat

# Message routing information
GET /router

# Performance metrics (NEW)
GET /performance

# Reconnection statistics
GET /reconnection
```

#### Model Management
```bash
# List available models
GET /models

# Clear model cache
POST /clear-cache
```

#### Core Transcription API
```bash
# Basic transcription (auto-model selection)
POST /transcribe
Content-Type: multipart/form-data
Body: file=audio.wav

# Model-specific transcription
POST /transcribe/{model_name}
Content-Type: multipart/form-data
Body: file=audio.wav, language=en, task=transcribe

# Enhanced transcription with audio processing
POST /transcribe/enhanced/{model_name}
Content-Type: multipart/form-data
Body: file=audio.wav, enhance_audio=true, noise_reduction=true
```

#### Streaming API
```bash
# Configure streaming parameters
POST /stream/configure
Content-Type: application/json
Body: {
  "chunk_size": 1024,
  "overlap": 0.1,
  "vad_enabled": true,
  "sample_rate": 16000
}

# Start streaming session
POST /stream/start
Content-Type: application/json
Body: {
  "model_name": "whisper-small.en",
  "language": "auto",
  "session_id": "optional-custom-id"
}

# Send audio chunk
POST /stream/audio
Content-Type: multipart/form-data
Body: audio_chunk=<binary_data>, session_id=<id>

# Get rolling transcriptions
GET /stream/transcriptions?session_id=<id>&limit=10

# Stop streaming session
POST /stream/stop
Content-Type: application/json
Body: {"session_id": "<id>"}
```

#### Session Management
```bash
# Create transcription session
POST /sessions
Content-Type: application/json
Body: {
  "model_name": "whisper-small.en",
  "language": "auto",
  "metadata": {"user_id": "123"}
}

# Get session details
GET /sessions/{session_id}

# Get session buffered messages (for reconnection)
GET /sessions/{session_id}/messages?limit=50

# Get detailed session info
GET /sessions/{session_id}/info

# Close session
DELETE /sessions/{session_id}
```

#### Authentication API
```bash
# User login
POST /auth/login
Content-Type: application/json
Body: {"username": "user", "password": "pass"}

# Create guest token
POST /auth/guest
Content-Type: application/json
Body: {"session_duration": 3600}

# Validate token
POST /auth/validate
Content-Type: application/json
Body: {"token": "jwt-token-here"}

# Logout
POST /auth/logout
Authorization: Bearer <token>

# Authentication statistics
GET /auth/stats
```

#### Pipeline Integration (Advanced)
```bash
# Pipeline transcription
POST /api/pipeline/transcribe
Content-Type: multipart/form-data
Body: file=audio.wav, session_id=<id>

# Start pipeline streaming
POST /api/pipeline/stream/start
Content-Type: application/json
Body: {"session_id": "pipeline-123"}

# Stop pipeline streaming
POST /api/pipeline/stream/stop/{session_id}

# Get pipeline status
GET /api/pipeline/status/{session_id}
```

## üîß Development

### Running Tests
```bash
# Unit tests
python -m pytest tests/unit/ -v

# Integration tests
python -m pytest tests/integration/ -v

# Load tests
python tests/load/load_test.py --concurrent=10 --duration=60
```

### Development with Hot Reload
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Start with hot reload
python src/main.py --reload --debug

# Or with Docker development setup
docker-compose -f docker-compose.dev.yml up --build
```

### Code Quality
```bash
# Linting
flake8 src/
black src/
isort src/

# Type checking
mypy src/

# Security scanning
bandit -r src/
```

## üìä Monitoring

### Metrics Endpoint
```bash
# Prometheus metrics
curl http://localhost:5001/metrics
```

Key metrics:
- `whisper_transcription_duration_seconds`
- `whisper_requests_total`
- `whisper_errors_total`
- `whisper_model_load_duration_seconds`

### Logging
Logs are written to:
- **Console**: JSON structured logs
- **File**: `logs/whisper-service.log` (if configured)
- **External**: Loki/ELK stack (if configured)

```bash
# View real-time logs
docker-compose logs -f whisper-service

# Local development logs
tail -f logs/whisper-service.log
```

## üöÄ Deployment Options

### Standalone Production
```bash
# Build production image
docker build -t my-whisper-service .

# Run in production mode
docker run -d \
  --name whisper-service \
  -p 5001:5001 \
  -v $(pwd)/models:/app/models \
  -e WHISPER_MODEL=whisper-medium.en \
  -e WHISPER_DEVICE=gpu \
  my-whisper-service
```

### Integration with LiveTranslate
```bash
# As part of full system
cd ../../  # Back to project root
docker-compose -f docker-compose.comprehensive.yml up whisper-service
```

### Kubernetes Deployment
```yaml
# k8s/whisper-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: whisper-service
  template:
    spec:
      containers:
      - name: whisper-service
        image: livetranslate/whisper-service:latest
        ports:
        - containerPort: 5001
        env:
        - name: WHISPER_MODEL
          value: "whisper-small.en"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
```

## üîç Troubleshooting

### Common Issues

**Service won't start:**
```bash
# Check port availability
netstat -tulpn | grep 5001

# Check Docker logs
docker-compose logs whisper-service

# Verify dependencies
pip check
```

**NPU not detected:**
```bash
# Check NPU drivers
python -c "import openvino; print('OpenVINO:', openvino.__version__)"

# List available devices
curl http://localhost:5001/api/devices
```

**Out of memory errors:**
```bash
# Use smaller model
WHISPER_MODEL=whisper-tiny.en

# Reduce concurrent requests
MAX_CONCURRENT_REQUESTS=1

# Monitor memory usage
docker stats whisper-service
```

**Poor transcription quality:**
```bash
# Use larger model
WHISPER_MODEL=whisper-medium.en

# Enable audio enhancement
ENABLE_ENHANCEMENT=true

# Check audio quality
ffprobe your_audio.wav
```

## ü§ù API Reference

### REST Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/health` | Service health check |
| GET | `/api/devices` | List available compute devices |
| GET | `/models` | List available models |
| POST | `/transcribe/{model}` | Single file transcription |
| POST | `/transcribe/enhanced/{model}` | Enhanced transcription |
| POST | `/stream/configure` | Configure streaming |
| POST | `/stream/audio` | Send audio chunk |
| GET | `/stream/transcriptions` | Get results |

### WebSocket Events

| Event | Direction | Description |
|-------|-----------|-------------|
| `configure` | Client ‚Üí Server | Set streaming parameters |
| `audio_chunk` | Client ‚Üí Server | Send audio data |
| `transcription` | Server ‚Üí Client | Transcription result |
| `error` | Server ‚Üí Client | Error notification |

## üìö Integration Examples

### With Translation Service
```python
import requests

# Transcribe
audio_response = requests.post(
    "http://localhost:5001/transcribe/whisper-small.en",
    files={"file": open("audio.wav", "rb")}
)
text = audio_response.json()["text"]

# Translate
translation_response = requests.post(
    "http://localhost:5003/translate",
    json={"text": text, "target_language": "Spanish"}
)
translated = translation_response.json()["translated_text"]
```

### With Speaker Service
```python
# Combined transcription and diarization
response = requests.post(
    "http://localhost:5001/transcribe/with-speakers/whisper-small.en",
    files={"file": open("audio.wav", "rb")},
    json={"enable_diarization": True}
)
result = response.json()
print(f"Speaker {result['speaker_id']}: {result['text']}")
```

---

## Overview

This module provides:
- **NPU-Optimized Transcription** using OpenVINO and NPU acceleration
- **Real-time Streaming** with rolling buffer management
- **Model Management** with automatic NPU device detection
- **Enhanced Audio Processing** with speech enhancement and VAD
- **Session Persistence** for transcriptions and settings
- **REST API** with comprehensive endpoints

## Core Components

### Extracted from `whisper-npu-server/server.py`:

```
whisper-service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_manager.py        # NPU model loading and management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_processor.py      # Audio preprocessing and enhancement  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcription_engine.py # Core transcription logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_manager.py      # Session persistence and settings
‚îÇ   ‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buffer_manager.py       # Rolling buffer for real-time streaming
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streaming_handler.py    # Stream management and inference
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference_scheduler.py  # Timed inference coordination
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcription_api.py    # /transcribe endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streaming_api.py        # /stream/* endpoints  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ management_api.py       # /models, /settings, /health
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py              # Main Flask application
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ audio_utils.py          # Audio format conversion, validation
‚îÇ       ‚îú‚îÄ‚îÄ device_detection.py     # NPU/GPU/CPU device detection
‚îÇ       ‚îî‚îÄ‚îÄ logging_utils.py        # Enhanced logging with activity logs
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ models.yml                  # Model configurations
‚îÇ   ‚îî‚îÄ‚îÄ settings.yml                # Default settings
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.npu              # Links to existing NPU Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml          # Standalone testing
‚îî‚îÄ‚îÄ requirements.txt                # Based on existing requirements
```

## Key Features

### NPU Acceleration
- **OpenVINO Integration**: Full NPU acceleration support
- **Device Auto-Detection**: Automatic fallback NPU ‚Üí GPU ‚Üí CPU
- **Model Management**: Dynamic loading with caching and optimization
- **Memory Management**: Efficient NPU cache clearing and memory handling

### Real-time Streaming  
- **Rolling Buffer**: 6-second windows with configurable overlap
- **Inference Scheduling**: Timed inference with overlap detection  
- **Stream Management**: Start/stop streaming with session persistence
- **Audio Chunking**: WebRTC VAD and smart speech boundary detection

### Audio Processing
- **Format Support**: WAV, MP3, FLAC, OGG via FFmpeg integration
- **Quality Enhancement**: Noise reduction and audio normalization
- **Voice Activity Detection**: WebRTC VAD with silence removal
- **Sample Rate Handling**: Automatic resampling to 16kHz

### Session Management
- **Persistent Sessions**: Transcription history and user settings  
- **Settings API**: Dynamic configuration updates
- **Activity Logging**: Comprehensive request and processing logs
- **State Recovery**: Automatic session restoration on restart

## API Endpoints

### Core Transcription
```bash
# Single file transcription  
POST /transcribe/<model_name>
POST /transcribe

# Enhanced transcription with preprocessing
POST /transcribe/enhanced/<model_name>
```

### Real-time Streaming
```bash
# Configure streaming parameters
POST /stream/configure

# Send audio chunks
POST /stream/audio  

# Get transcription results
GET /stream/transcriptions

# Control streaming
POST /stream/start
POST /stream/stop
```

### Management
```bash
# Health and status
GET /health

# Model management  
GET /models
POST /clear-cache

# Settings management
GET /settings
POST /settings
POST /settings/<setting_key>

# Server control
POST /restart
POST /shutdown
```

## Configuration

### Model Settings
```yaml
# config/models.yml
models:
  default: "whisper-small-en"
  available:
    - "whisper-tiny-en"
    - "whisper-base-en" 
    - "whisper-small-en"
    - "whisper-medium-en"
  device_priority: ["NPU", "GPU", "CPU"]
  cache_size: 3
```

### Streaming Settings
```yaml
# config/settings.yml
streaming:
  buffer_duration: 6.0      # seconds
  inference_interval: 3.0   # seconds
  overlap_threshold: 0.3    # overlap detection
  sample_rate: 16000        # Hz
  
audio:
  enable_enhancement: true
  noise_reduction: true
  normalize_volume: true
  vad_enabled: true
```

## Integration Points

### With Speaker Diarization Module
```python
from modules.speaker_service import SpeakerDiarizationService

# Enhanced transcription with speaker identification
whisper_service = WhisperService()
speaker_service = SpeakerDiarizationService()

result = await whisper_service.transcribe_with_speakers(
    audio_data, 
    speaker_service=speaker_service
)
```

### With Translation Module  
```python
from modules.translation_service import TranslationService

# Transcribe and translate in one call
translation_service = TranslationService()

result = await whisper_service.transcribe_and_translate(
    audio_data,
    target_language="Spanish",
    translation_service=translation_service
)
```

## Usage Examples

### Basic Transcription
```python
from modules.whisper_service import WhisperService

# Initialize service
service = WhisperService()
await service.initialize()

# Transcribe audio file
result = await service.transcribe(
    audio_file="speech.wav",
    model="whisper-small-en"
)

print(result.text)
print(f"Confidence: {result.confidence}")
print(f"Processing time: {result.latency_ms}ms")
```

### Real-time Streaming
```python
# Configure streaming
await service.configure_streaming(
    buffer_duration=6.0,
    inference_interval=3.0
)

# Start streaming session
await service.start_streaming()

# Send audio chunks
async for audio_chunk in audio_stream:
    await service.process_audio_chunk(audio_chunk)

# Get results
transcriptions = await service.get_rolling_transcriptions()
```

### Enhanced Processing
```python
# Transcription with enhancement
result = await service.transcribe_enhanced(
    audio_data,
    enable_noise_reduction=True,
    enable_volume_normalization=True,
    vad_enabled=True
)
```

## Performance Characteristics

### NPU Performance
- **Latency**: 100-300ms for 6-second audio chunks
- **Throughput**: Real-time processing with 2x speedup
- **Memory**: ~2GB NPU memory usage for small models
- **Power**: Optimized for low power consumption

### Fallback Performance  
- **GPU**: 200-500ms latency, higher throughput
- **CPU**: 1-3s latency, reliable fallback
- **Auto-detection**: <100ms device detection time

## Migration from Existing Code

### Preserved Functionality
‚úÖ All existing endpoints work unchanged  
‚úÖ Session persistence and settings  
‚úÖ NPU acceleration and model management  
‚úÖ Real-time streaming with rolling buffers  
‚úÖ Audio format support and enhancement  
‚úÖ CORS support and request logging

### New Modular Benefits
üÜï Independent testing and deployment  
üÜï Clean separation of concerns  
üÜï Integration with other modules  
üÜï Enhanced monitoring and metrics  
üÜï Simplified configuration management  

### Migration Steps
1. **Phase 1**: Extract core components into modules
2. **Phase 2**: Create unified service interface  
3. **Phase 3**: Add integration hooks for other modules
4. **Phase 4**: Enhanced monitoring and observability

This preserves all your excellent NPU optimization work while making it modular and easier to integrate! 