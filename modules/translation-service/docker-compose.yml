services:
  # vLLM Translation Server with GPU acceleration (Primary Service)
  vllm-translation:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: livetranslate-vllm-translation
    ports:
      - "8010:8010"  # REST API server (HTTP)
      - "8011:8011"  # WebSocket server
      - "5003:5003"  # Legacy compatibility port
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/transformers
      - VLLM_CACHE_ROOT=/app/cache/vllm
      - TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0"
      - CUDA_HOME=/usr/local/cuda
      - MODEL_NAME=Qwen/Qwen2.5-14B-Instruct-AWQ
      - MAX_MODEL_LEN=2048
      - GPU_MEMORY_UTILIZATION=0.9
      - TENSOR_PARALLEL_SIZE=1
      # Logging
      - LOG_LEVEL=INFO
      # Security
      - SECRET_KEY=${SECRET_KEY:-dev-secret-key-change-in-production}
    volumes:
      - vllm_models:/app/models
      - vllm_cache:/app/cache
      - ./logs:/app/logs
      - ./config:/app/config
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - translation-network
    # depends_on:
    #   - redis  # Using external Redis
    profiles:
      - gpu
      - default

  # Model downloader service (run once to download models)
  model-downloader:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: livetranslate-model-downloader
    environment:
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/transformers
      - MODEL_NAME=Qwen/Qwen2.5-14B-Instruct-AWQ
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - vllm_models:/app/models
      - vllm_cache:/app/cache
    command: python src/model_downloader.py --model Qwen/Qwen2.5-14B-Instruct-AWQ --cache-dir /app/cache
    profiles:
      - download-models
    networks:
      - translation-network

  # Main translation service (CPU/API fallback)
  translation-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: livetranslate-translation-api
    ports:
      - "5003:5003"  # Main API service port
    environment:
      # CPU-specific settings
      - USE_LOCAL_INFERENCE=true
      - USE_LEGACY_APIS=false
      - USE_MOCK_TRANSLATION=false

      # Model configuration for CPU
      - TRANSLATION_MODEL=Qwen/Qwen2.5-7B-Instruct-AWQ
      - MAX_TOKENS=512
      - TEMPERATURE=0.1

      # Service settings
      - TRANSLATION_TIMEOUT=60  # Longer timeout for CPU
      - RETRY_ATTEMPTS=3
      - CONFIDENCE_THRESHOLD=0.8

      # Ollama settings (CPU inference)
      - OLLAMA_BASE_URL=http://ollama:11434

      # External API keys (fallback)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}

      # Logging
      - LOG_LEVEL=INFO

      # Security
      - SECRET_KEY=${SECRET_KEY:-dev-secret-key-change-in-production}

    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
    restart: unless-stopped
    networks:
      - translation-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      - ollama
      # - redis  # Using external Redis
    profiles:
      - api
      - default

  # Development/testing server with mock translation
  translation-dev:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: livetranslate-translation-dev
    ports:
      - "5005:5003"  # Development port
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - USE_MOCK_TRANSLATION=true
      - LOG_LEVEL=DEBUG
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
    command: python src/vllm_server_simple.py --host 0.0.0.0 --port 8010
    profiles:
      - dev
    networks:
      - translation-network
    # depends_on:
    #   - redis  # Using external Redis

  # Service Integration Layer
  integration-service:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: livetranslate-integration
    environment:
      - TRANSLATION_HOST=vllm-translation
      - TRANSLATION_PORT=8010
      - WHISPER_HOST=whisper-service
      - WHISPER_PORT=5001
      - SPEAKER_HOST=speaker-service
      - SPEAKER_PORT=5002
      - FRONTEND_HOST=frontend-service
      - FRONTEND_PORT=3000
      - LOG_LEVEL=INFO
    volumes:
      - ./logs:/app/logs
    command: python src/service_integration.py
    networks:
      - translation-network
    depends_on:
      - vllm-translation
      # - redis  # Using external Redis
    profiles:
      - integration
      - full-system

  # Redis for session management and caching (external - assumes Redis is running on host)
  # Note: Redis is assumed to be running externally (e.g., from whisper-service)
  # If you need a local Redis, uncomment the section below and change depends_on
  #redis:
  #  image: redis:7-alpine
  #  container_name: livetranslate-translation-redis-local
  #  ports:
  #    - "6380:6379"  # Use different host port to avoid conflicts
  #  volumes:
  #    - redis_data:/data
  #  command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
  #  restart: unless-stopped
  #  networks:
  #    - translation-network

  # Optional: Ollama service for CPU inference
  ollama:
    image: ollama/ollama:latest
    container_name: livetranslate-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    networks:
      - translation-network
    environment:
      - OLLAMA_HOST=0.0.0.0
    profiles:
      - cpu

networks:
  translation-network:
    driver: bridge
    name: livetranslate-network

volumes:
  vllm_models:
    driver: local
  vllm_cache:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local

# Usage examples:
#
# Download models first:
# docker-compose --profile download-models up model-downloader
#
# Start GPU server (vLLM with CUDA):
# docker-compose --profile gpu up -d
#
# Start API server (Flask with external APIs):
# docker-compose --profile api up -d
#
# Start development server:
# docker-compose --profile dev up -d
#
# Start with integration layer:
# docker-compose --profile integration up -d
#
# Start full system (for complete integration testing):
# docker-compose --profile full-system up -d
#
# Default (API server):
# docker-compose up -d
#
# Check logs:
# docker-compose logs -f vllm-translation
# docker-compose logs -f integration-service
#
# Test API:
# curl http://localhost:8010/health
# curl -X POST http://localhost:8010/translate -H "Content-Type: application/json" -d '{"text": "Hello world"}'
#
# Test integration:
# docker-compose exec integration-service python src/service_integration.py --test-only
#
# Stop all services:
# docker-compose down
