services:
  # Triton-based Translation Server with vLLM backend
  triton-translation:
    build:
      context: .
      dockerfile: Dockerfile.triton
      args:
        TRITON_VERSION: "24.12"
    container_name: livetranslate-triton-translation
    ports:
      - "8000:8000"  # Triton HTTP inference
      - "8001:8001"  # Triton gRPC inference
      - "8002:8002"  # Triton metrics
      - "5003:5003"  # Translation service API
    environment:
      # Model configuration
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
      - TENSOR_PARALLEL_SIZE=1
      - MAX_MODEL_LEN=4096
      - GPU_MEMORY_UTILIZATION=0.9
      - ENFORCE_EAGER=false

      # Triton configuration
      - TRITON_MODEL_REPOSITORY=/app/model_repository
      - TRITON_HTTP_PORT=8000
      - TRITON_GRPC_PORT=8001
      - TRITON_METRICS_PORT=8002

      # Translation service configuration
      - TRANSLATION_SERVICE_PORT=5003
      - TRITON_BASE_URL=http://localhost:8000
      - INFERENCE_BACKEND=triton

      # CUDA configuration
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Cache directories
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/transformers
      - TRITON_CACHE_ROOT=/app/cache/triton
    volumes:
      # Model cache persistence
      - triton_models:/app/cache
      - triton_logs:/app/logs

      # Optional: Mount local model repository for development
      # - ./triton-config:/app/model_repository:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health", "&&", "curl", "-f", "http://localhost:5003/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - livetranslate-triton

  # Redis for caching (optional but recommended)
  redis-triton:
    image: redis:7-alpine
    container_name: livetranslate-triton-redis
    ports:
      - "6379:6379"
    volumes:
      - triton_redis_data:/data
    restart: unless-stopped
    networks:
      - livetranslate-triton

  # Prometheus for metrics collection (optional)
  prometheus-triton:
    image: prom/prometheus:latest
    container_name: livetranslate-triton-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - triton_prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - livetranslate-triton

volumes:
  triton_models:
    driver: local
  triton_logs:
    driver: local
  triton_redis_data:
    driver: local
  triton_prometheus_data:
    driver: local

networks:
  livetranslate-triton:
    driver: bridge
    name: livetranslate-triton-network
