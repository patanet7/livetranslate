# ==============================================================================
# LiveTranslate Translation Service Configuration
# ==============================================================================
#
# This service provides:
# - Multi-language translation with LLM backends
# - OpenAI-compatible API support (Ollama, Groq, vLLM, etc.)
# - GPU-optimized inference
# - Translation caching with Redis
# - Quality scoring and validation
#

# ==============================================================================
# Server Configuration
# ==============================================================================

# Service host and port
HOST=0.0.0.0

# ==============================================================================
# Device Configuration (Hardware Acceleration)
# ==============================================================================

# Primary device: gpu, cpu, auto
DEVICE=auto

# CUDA device ID (for multi-GPU systems)
# CUDA_VISIBLE_DEVICES=0

# GPU memory utilization limit (0.0 - 1.0)
GPU_MEMORY_UTILIZATION=0.85

# ==============================================================================
# OpenAI-Compatible Translation Backends
# ==============================================================================
# The translation service now supports ANY OpenAI-compatible API endpoint!
# This includes: Ollama, Groq, Together AI, vLLM, OpenRouter, and many more.
#
# Priority order (if multiple enabled):
#   Ollama → Groq → Together AI → vLLM Server → OpenAI → Custom
# ----------------------------------------------------------------------------

# ============================================================================
# OLLAMA (Local - Recommended for Privacy & Cost)
# ============================================================================
# Run locally: ollama serve
# Download models: ollama pull llama3.1:8b

OLLAMA_ENABLE=true
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1:8b
# Alternatives: mistral:7b, codellama:13b, phi3:latest

# ============================================================================
# GROQ (Cloud - Ultra Fast & Free Tier)
# ============================================================================
# Get API key from: https://console.groq.com/
# Free tier: 14,400 requests/day with llama-3.1-8b-instant

GROQ_ENABLE=false
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.1-8b-instant
# Alternatives:
#   - llama-3.1-70b-versatile (higher quality, slower)
#   - mixtral-8x7b-32768 (long context)
#   - gemma2-9b-it (good for translation)

# ============================================================================
# TOGETHER AI (Cloud - Wide Model Selection)
# ============================================================================
# Get API key from: https://api.together.xyz/
# Pay-per-use pricing, good model variety

TOGETHER_ENABLE=false
TOGETHER_API_KEY=your_together_api_key_here
TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
# Alternatives:
#   - meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
#   - mistralai/Mixtral-8x7B-Instruct-v0.1
#   - Qwen/Qwen2-72B-Instruct

# ============================================================================
# OPENAI (Cloud - Highest Quality)
# ============================================================================
# Get API key from: https://platform.openai.com/
# Most expensive but highest quality

OPENAI_ENABLE=false
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
# Alternatives:
#   - gpt-4o (best quality, most expensive)
#   - gpt-4o-mini (good balance)
#   - gpt-3.5-turbo (cheapest)

# ============================================================================
# vLLM SERVER (Self-Hosted - High Performance)
# ============================================================================
# Run your own vLLM server:
#   python -m vllm.entrypoints.openai.api_server \
#     --model meta-llama/Meta-Llama-3.1-8B-Instruct \
#     --port 8000

VLLM_SERVER_ENABLE=false
VLLM_SERVER_BASE_URL=http://localhost:8000/v1
VLLM_SERVER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# ============================================================================
# CUSTOM OPENAI-COMPATIBLE ENDPOINT
# ============================================================================
# Any service that implements OpenAI's API format:
# - OpenRouter (https://openrouter.ai/)
# - Replicate (https://replicate.com/)
# - Fireworks AI (https://fireworks.ai/)
# - LM Studio (local GUI)
# - LocalAI (https://localai.io/)
# - Text Generation Inference (HuggingFace)
# - Jan.ai (local)
# - And many more!

CUSTOM_OPENAI_ENABLE=false
CUSTOM_OPENAI_NAME=my-custom-service
CUSTOM_OPENAI_BASE_URL=http://my-server.com/v1
CUSTOM_OPENAI_MODEL=my-model-name
CUSTOM_OPENAI_API_KEY=optional_api_key

# ============================================================================
# LEGACY BACKENDS (Still Supported)
# ============================================================================

# Local Transformers Models (requires GPU/CPU inference)
TRANSLATION_MODEL=./models/Llama-3.1-8B-Instruct
GPU_ENABLE=true

# ============================================================================
# GENERAL SETTINGS
# ============================================================================

# Service Port
PORT=5003

# Translation Cache (Redis)
# Requires: REDIS_URL set in orchestration service
TRANSLATION_CACHE_ENABLED=true
TRANSLATION_CACHE_TTL=3600

# Logging
LOG_LEVEL=INFO

# Performance
MAX_BATCH_SIZE=32
MAX_TOKENS=2048
TEMPERATURE=0.3

# ============================================================================
# EXAMPLE CONFIGURATIONS
# ============================================================================

# ── Local Development (Free) ───────────────────────────────────────────────
# OLLAMA_ENABLE=true
# Everything else disabled
# → 100% private, 100% free, requires local GPU/CPU

# ── Production (High Quality) ──────────────────────────────────────────────
# GROQ_ENABLE=true (primary - fast and free tier)
# OPENAI_ENABLE=true (fallback - highest quality)
# → Fast responses with quality fallback

# ── Production (Cost-Optimized) ────────────────────────────────────────────
# OLLAMA_ENABLE=true (primary - local)
# GROQ_ENABLE=true (fallback - free tier cloud)
# → Minimize costs while maintaining availability

# ── Enterprise (Self-Hosted) ───────────────────────────────────────────────
# VLLM_SERVER_ENABLE=true (primary)
# OLLAMA_ENABLE=true (fallback)
# → Full control, high performance

# ============================================================================
# QUICK START EXAMPLES
# ============================================================================

# 1. Local Ollama Setup (Easiest):
#    ollama pull llama3.1:8b
#    cp .env.example .env
#    # Set OLLAMA_ENABLE=true in .env
#    python src/api_server.py

# 2. Groq Cloud Setup (Fast & Free):
#    # Get API key from https://console.groq.com/
#    cp .env.example .env
#    # Set GROQ_ENABLE=true and GROQ_API_KEY in .env
#    python src/api_server.py

# 3. Multiple Backends (Recommended):
#    # Enable both Ollama (local) and Groq (cloud) for redundancy
#    # Set both OLLAMA_ENABLE=true and GROQ_ENABLE=true
#    python src/api_server.py

# ============================================================================
# TESTING YOUR CONFIGURATION
# ============================================================================
# After starting the service, check available models:
#   curl http://localhost:5003/api/models/available
#
# Test translation with specific backend:
#   curl -X POST http://localhost:5003/api/translate/multi \
#     -H "Content-Type: application/json" \
#     -d '{
#       "text": "Hello world",
#       "target_languages": ["es", "fr"],
#       "model": "ollama"
#     }'
#
# Test with auto-selection:
#   curl -X POST http://localhost:5003/api/translate/multi \
#     -H "Content-Type: application/json" \
#     -d '{
#       "text": "Hello world",
#       "target_languages": ["es", "fr"],
#       "model": "auto"
#     }'


# ==============================================================================
# SERVICE INTEGRATION
# ==============================================================================

# Orchestration service URL (for service discovery)
ORCHESTRATION_URL=http://localhost:3000

# Whisper service URL (for transcription integration)
WHISPER_URL=http://localhost:5001

# Enable service registration with orchestration
REGISTER_WITH_ORCHESTRATION=true

# Health check interval (seconds)
HEALTH_CHECK_INTERVAL=30


# ==============================================================================
# REDIS CONFIGURATION
# ==============================================================================

# Redis URL for translation caching
REDIS_URL=redis://localhost:6379/1

# Cache key prefix
CACHE_PREFIX=livetranslate:translation:


# ==============================================================================
# QUALITY & VALIDATION
# ==============================================================================

# Minimum translation quality threshold (0.0 - 1.0)
QUALITY_THRESHOLD=0.7

# Enable translation validation
VALIDATION_ENABLED=true

# Maximum retries on translation failure
MAX_RETRIES=3

# Retry delay (seconds)
RETRY_DELAY=1.0


# ==============================================================================
# RATE LIMITING
# ==============================================================================

# Rate limit (requests per minute)
RATE_LIMIT_RPM=1000

# Burst limit
RATE_LIMIT_BURST=50


# ==============================================================================
# API SETTINGS
# ==============================================================================

# Maximum text length per request (characters)
MAX_TEXT_LENGTH=10000

# Request timeout (seconds)
REQUEST_TIMEOUT=60

# Maximum concurrent translation requests
MAX_CONCURRENT_REQUESTS=50
