# LiveTranslate Development Guide

## Project Overview

**LiveTranslate** is a comprehensive, production-ready system for real-time audio transcription and translation using AI acceleration. It provides enterprise-grade features including NPU/GPU acceleration, speaker diarization, multi-language translation, and distributed deployment capabilities.

### Key Technologies
- **Languages**: Python (primary), C++ (legacy audio processing), JavaScript (frontend)
- **Frameworks**: Flask, Flask-SocketIO, WebSockets, Docker
- **AI/ML**: OpenAI Whisper, vLLM, Ollama, OpenVINO, PyTorch
- **Infrastructure**: Docker Compose, Redis, PostgreSQL, Prometheus, Grafana
- **Hardware Acceleration**: Intel NPU (preferred), NVIDIA GPU, CPU fallback

### Architecture
- **Microservices**: Modular architecture with independent services
- **Real-time**: WebSocket-based communication for streaming
- **Scalable**: Horizontal scaling with load balancing support
- **Monitoring**: Comprehensive observability stack

## Directory Structure

### Core Modules (`modules/`)
- **`frontend-service/`** - Web interface and API gateway (Port 3000)
- **`whisper-service/`** - NPU/GPU-accelerated speech-to-text (Port 5001)
- **`speaker-service/`** - Speaker diarization and identification (Port 5002)
- **`translation-service/`** - Local LLM translation with GPU preference (Port 5003)
- **`websocket-service/`** - Enterprise WebSocket infrastructure (Port 8765)
- **`monitoring-service/`** - Prometheus, Grafana, Loki stack
- **`shared/`** - Common utilities and inference abstractions

### Legacy Code (`legacy/`)
- **`python/`** - Original Python implementation and NPU server
- **`cpp/`** - C++ audio processing components
- **`.roo/`** - Legacy development rules and workflows

### Configuration & Scripts
- **`scripts/`** - Deployment, security, and management scripts
- **`.taskmaster/`** - Task management and project planning
- **`.cursor/`** - Development workflow rules and guidelines

## Development Workflow

### Environment Setup
```bash
# Copy environment template
cp env.template .env

# Edit .env with your configuration
# Key settings: NPU/GPU detection, model paths, API keys

# Deploy with hardware detection
python scripts/deploy.py

# Or manual deployment
docker-compose -f docker-compose.comprehensive.yml up -d
```

### Service Development
- Each service in `modules/` can be developed independently
- Use `docker-compose up --build` for individual service testing
- Services follow standardized patterns:
  - Health endpoints at `/api/health`
  - Prometheus metrics support
  - Structured JSON logging
  - Environment-based configuration

### Hardware Acceleration Preferences
1. **Whisper Service**: NPU > GPU > CPU
2. **Translation Service**: GPU > CPU (memory intensive)
3. **Speaker Service**: CPU (moderate requirements)
4. **Frontend Service**: CPU (lightweight)

## Best Practices

### Code Organization
- **Modular Design**: Each service is self-contained with clear interfaces
- **Async/Await**: Use async patterns for I/O operations
- **Type Hints**: Use dataclasses and type annotations
- **Error Handling**: Comprehensive error handling with fallbacks
- **Configuration**: Environment-based configuration with sensible defaults

### Docker Practices
- **Multi-stage Builds**: Optimize image sizes
- **Non-root Users**: Security-first container design
- **Health Checks**: All services implement health monitoring
- **Resource Limits**: Appropriate CPU/memory constraints
- **Secrets Management**: Use Docker secrets for sensitive data

### Performance Optimization
- **Model Caching**: Persistent volumes for AI models
- **Connection Pooling**: Redis and database connections
- **Streaming**: Real-time processing with rolling buffers
- **Hardware Detection**: Automatic NPU/GPU detection and optimization

### Security Guidelines
- **Network Isolation**: Separate Docker networks for different layers
- **Secrets**: Never commit API keys or passwords
- **CORS**: Proper cross-origin configuration
- **Authentication**: JWT-based authentication where needed
- **Container Hardening**: Use security scripts in `scripts/`

## Service Communication Patterns

### WebSocket Real-time Streams
```python
# Frontend <-> All Services
# Real-time transcription, translation, speaker updates
# Session management and state synchronization
```

### HTTP/REST APIs
```python
# Service discovery and health checks
# Configuration management
# Batch processing requests
```

### Message Queues (Redis)
```python
# Asynchronous task processing
# Service coordination
# Session state sharing
```

## Testing Strategy

### Unit Testing
```bash
# Individual service testing
cd modules/whisper-service && python -m pytest tests/unit/
cd modules/speaker-service && python -m pytest tests/unit/
cd modules/translation-service && python -m pytest tests/unit/
```

### Integration Testing
```bash
# End-to-end workflow testing
python tests/integration/test_complete_pipeline.py

# Service communication testing
python tests/integration/test_service_communication.py
```

### Load Testing
```bash
# Individual service load testing
python tests/load/test_whisper_load.py --concurrent=50 --duration=300s

# System-wide load testing
python tests/load/test_system_load.py --clients=100 --duration=600s
```

## Deployment Scenarios

### Single Machine Development
```bash
# All services on one machine
# 16GB+ RAM recommended
# Modern CPU with AVX support
# Optional: GPU/NPU for acceleration
```

### Distributed Production
```bash
# Machine 1: Whisper Service (NPU/GPU accelerated)
# Machine 2: Translation Service (High-memory)
# Machine 3: Frontend & Supporting Services
```

### Kubernetes
```yaml
# Horizontal Pod Autoscaling (HPA)
# Service mesh integration
# Ingress configuration
# Persistent volume claims
```

## Monitoring & Observability

### Health Monitoring
```bash
# Individual service health
curl http://localhost:5001/api/health

# Aggregated system health
curl http://localhost:3000/api/system/health
```

### Metrics Collection
- **Prometheus**: Service-specific and system metrics
- **Grafana**: Visualization dashboards
- **Loki**: Centralized logging
- **AlertManager**: Automated alerting

## Configuration Management

### Environment Variables
- **Hardware Detection**: `LIVETRANSLATE_NPU_ENABLED`, `LIVETRANSLATE_GPU_ENABLED`
- **Inference Backends**: `INFERENCE_BACKEND`, `VLLM_BASE_URL`, `OLLAMA_BASE_URL`
- **Model Paths**: `WHISPER_MODELS_PATH`, `HUGGINGFACE_CACHE_PATH`
- **Service URLs**: Internal Docker network communication
- **Security**: JWT secrets, CORS settings, SSL configuration

### Service Discovery
```bash
# Static configuration
WHISPER_SERVICE_URL=http://whisper-host:5001
SPEAKER_SERVICE_URL=http://speaker-host:5002

# Dynamic discovery (Kubernetes)
# Load balancer endpoints
```

## Development Rules Integration

### Cursor IDE Rules
- Follow `.cursor/rules/` for development workflow
- Use TaskMaster for project management
- Implement self-improvement patterns
- Windows PowerShell environment considerations

### Legacy Rules Migration
- Python rules from `legacy/.roo/rules/python_rules.md`
- C++ rules from `legacy/.roo/rules/cpp_rules.md`
- Interactive review protocols for quality assurance

## Common Commands

### Development
```bash
# Start all services
docker-compose -f docker-compose.comprehensive.yml up -d

# Work on specific service
cd modules/whisper-service && docker-compose up --build

# Run tests
python -m pytest tests/

# Check service health
curl http://localhost:3000/api/health
```

### Deployment
```bash
# Hardware detection and deployment
python scripts/deploy.py

# Security hardening
powershell scripts/harden-containers.ps1

# Monitoring deployment
powershell scripts/deploy-monitoring.ps1
```

### Maintenance
```bash
# Backup volumes
powershell scripts/backup-volumes.ps1

# Security check
powershell scripts/security-check.ps1

# Integration testing
powershell scripts/test-integration.ps1
```

## Troubleshooting

### Common Issues
- **NPU Detection**: Check OpenVINO installation and device availability
- **GPU Memory**: Monitor VRAM usage for translation service
- **WebSocket Connections**: Verify network configuration and CORS settings
- **Model Loading**: Check model cache paths and permissions

### Performance Tuning
- **Whisper Service**: Adjust model size based on hardware capabilities
- **Translation Service**: Configure vLLM memory settings for GPU
- **Speaker Service**: Tune clustering parameters for accuracy
- **Frontend Service**: Optimize WebSocket connection pooling

## Contributing

### Adding New Services
1. Follow established module structure in `modules/`
2. Implement standard health endpoints
3. Add comprehensive documentation
4. Include Docker configuration
5. Add monitoring and metrics
6. Write unit and integration tests

### Service Standards
- **Health Endpoints**: `/api/health` with standardized response
- **Metrics**: Prometheus-compatible metrics endpoint
- **Logging**: Structured JSON logging with correlation IDs
- **Configuration**: Environment-based configuration
- **Documentation**: Comprehensive README with examples

---
*This workflow provides a general guideline. Adapt it based on your specific project needs and team practices.*
This project emphasizes production-ready, enterprise-grade real-time AI processing with flexible deployment options and comprehensive monitoring. The modular architecture ensures maintainability while supporting both development and production environments.

